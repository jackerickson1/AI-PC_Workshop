{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c54c45-b57f-4c18-85cf-13af39ee4b0f",
   "metadata": {},
   "source": [
    "# Music Generation with Auto-regressive Transformer Model\n",
    "\n",
    "This module is based on the OpenVINO notebook [Controllable Music Generation with MusicGen and OpenVINOâ„¢](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/music-generation)\n",
    "\n",
    "If you are running this on your own, not as part of a workshop, install packages using `requirements.txt` and run the `setup.py` script before using this notebook.\n",
    "\n",
    "Start with the `05a_convert_models.ipynb` notebook. This converts the text encoder and audio decoder models to use static shapes for the inputs, which is required for running them on the NPU. It also shows different approachs for converting to static shapes.\n",
    "\n",
    "This notebook then loads in the MusicGen pipeline, tests it out so you can see how it works, then replaces the original text encoder and audio decoder models with the statically-shaped versions so they can run on the NPU. The device settings are hard-coded, feel free to try different devices. The core MusicGen \"mg\" model was not converted to static shapes just to keep things simple for this module.\n",
    "\n",
    "### MusicGen\n",
    "\n",
    "MusicGen is a single-stage auto-regressive Transformer model capable of generating high-quality music samples conditioned on text descriptions or audio prompts. The text prompt is passed to a text encoder model (T5) to obtain a sequence of hidden-state representations. These hidden states are fed to MusicGen, which predicts discrete audio tokens (audio codes). Finally, audio tokens are then decoded using an audio compression model (EnCodec) to recover the audio waveform.\n",
    "\n",
    "![pipeline](https://user-images.githubusercontent.com/76463150/260439306-81c81c8d-1f9c-41d0-b881-9491766def8e.png)\n",
    "\n",
    "[The MusicGen model](https://arxiv.org/abs/2306.05284) does not require a self-supervised semantic representation of the text/audio prompts; it operates over several streams of compressed discrete music representation with efficient token interleaving patterns, thus eliminating the need to cascade multiple models to predict a set of codebooks (e.g. hierarchically or upsampling). Unlike prior models addressing music generation, it is able to generate all the codebooks in a single forward pass.\n",
    "\n",
    "We will use a model implementation from the [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db65e62-6327-4594-8d0b-28c733a6e697",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9252c-a1e6-4eb2-b74d-feeefcdaf95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import MusicgenProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "import mg_utils as utils\n",
    "\n",
    "from torch.jit import TracerWarning\n",
    "import warnings\n",
    "# Ignore tracing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "original_model_path = models_dir / \"musicgen-small\"\n",
    "processor_path = models_dir / \"musicgen-small-processor\"\n",
    "t5_dynamic_ir_path = models_dir / \"t5.xml\"\n",
    "t5_static_ir_path = models_dir / \"t5_static.xml\"\n",
    "musicgen_0_ir_path = models_dir / \"mg_0.xml\"\n",
    "musicgen_ir_path = models_dir / \"mg.xml\"\n",
    "audio_decoder_dynamic_ir_path = models_dir / \"encodec.xml\"\n",
    "max_prompt_length = 100 # tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38da781-233d-413f-bcc1-45d44b15e9ba",
   "metadata": {},
   "source": [
    "### Original Pipeline Inference\n",
    "\n",
    "Text Preprocessing prepares the text prompt to be fed into the model, the `processor` object abstracts this step for us. Text tokenization is performed under the hood, it assigning tokens or IDs to the words; in other words, token IDs are just indices of the words in the model vocabulary. It helps the model understand the context of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28199a-219f-4a05-bdbc-9f0e6013a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import time\n",
    "\n",
    "loading_kwargs = {}\n",
    "# If transformers version is >= 4.40.0, add the following:\n",
    "loading_kwargs[\"attn_implementation\"] = \"eager\" \n",
    "\n",
    "# Load the pipeline\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(original_model_path, torchscript=True, return_dict=False, **loading_kwargs)\n",
    "processor = MusicgenProcessor.from_pretrained(processor_path)\n",
    "\n",
    "sample_length = 10  # seconds\n",
    "\n",
    "frame_rate = model.config.audio_encoder.frame_rate\n",
    "n_tokens = sample_length * frame_rate + 3\n",
    "print(f\"Each second of output music requires generating {frame_rate} tokens\")\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "print(\"Audio sampling rate is\", sampling_rate, \"Hz\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval();\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "start_time=time.time()\n",
    "# Test the pipeline using the above prompt. Generate 8 seconds (the model generates 50 tokens for each second)\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=n_tokens)\n",
    "print(f\"time={time.time()-start_time}\")\n",
    "generated_music = audio_values[0].cpu().numpy()\n",
    "\n",
    "Audio(generated_music, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02001557-7de5-494f-9572-8687d168dcf2",
   "metadata": {},
   "source": [
    "## Create a spectrogram of the generated music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f2d22-c665-482c-96e2-a3a4a4632877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# Compute the Short-Time Fourier Transform (STFT)\n",
    "frequencies, times, Sxx = spectrogram(generated_music, fs=sampling_rate)\n",
    "\n",
    "# Squeeze the Sxx array to ensure it's 2D\n",
    "Sxx = np.squeeze(Sxx)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx), shading='gouraud')\n",
    "plt.colorbar(label='Intensity (dB)')\n",
    "plt.title('STFT-based Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb1fe5-d144-4c95-b428-004711ac53a2",
   "metadata": {},
   "source": [
    "## Embedding the converted models into the original pipeline\n",
    "### Adapt OpenVINO models to the original pipeline\n",
    "\n",
    "Here we create wrapper classes for the OpenVINO models that we want to embed in the original inference pipeline.\n",
    "Here are some of the things to consider when adapting an OV model:\n",
    " - Make sure that parameters passed by the original pipeline are forwarded to the compiled OV model properly; sometimes the OV model uses only a portion of the input arguments and some are ignored, sometimes you need to convert the argument to another data type or unwrap some data structures such as tuples or dictionaries.\n",
    " - Guarantee that the wrapper class returns results to the pipeline in an expected format. In the example below you can see how we pack OV model outputs into special classes declared in the HF repo.\n",
    " - Pay attention to the model method used in the original pipeline for calling the model - it may be not the `forward` method! Refer to the `AudioDecoderWrapper` to see how we wrap OV model inference into the `decode` method.\n",
    "\n",
    "Note that for this notebook, we will defer embedding of the audio_decoder to the generate() function, since the user interface will provide a choice of the length of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e44f7a-74cf-4a25-972a-f90d3e1c3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_device = \"CPU\"\n",
    "audio_decoder_device = \"HETERO:GPU,NPU\"\n",
    "\n",
    "text_encode_ov = utils.TextEncoderWrapper(t5_static_ir_path, model.text_encoder.config, t5_device)\n",
    "\n",
    "del model.text_encoder\n",
    "gc.collect()\n",
    "\n",
    "model.text_encoder = text_encode_ov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a181fad-d3b9-48fc-8259-4fa0bba41e78",
   "metadata": {},
   "source": [
    "## Try out the converted pipeline\n",
    "\n",
    "We can now infer the pipeline backed by OpenVINO models. Note that with statically-shaped models, we need to explicitly set the padding to our max token length.\n",
    "\n",
    "The demo app below is created using [Gradio package](https://www.gradio.app/docs/interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71be63-b35d-417a-be19-2fc9980bf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, output_length):\n",
    "    global model, processor, max_prompt_length, sampling_rate, frame_rate\n",
    "\n",
    "    n_tokens_to_generate = output_length * frame_rate + 3\n",
    "    \n",
    "    inputs = processor(text=[prompt],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_prompt_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Plug in the appropriate audio_decoder model based on the desired output length\n",
    "    audio_decoder_static_ir_path = f\"{models_dir}/encodec_{output_length}s_ir.xml\"\n",
    "    audio_encoder_ov = utils.AudioDecoderWrapper(\n",
    "        audio_decoder_static_ir_path, \n",
    "        model.audio_encoder.config,\n",
    "        audio_decoder_device,\n",
    "    )\n",
    "    del model.audio_encoder\n",
    "    gc.collect()\n",
    "    model.audio_encoder = audio_encoder_ov\n",
    "    \n",
    "    audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=n_tokens_to_generate, use_cache=True)\n",
    "    # Convert the output to the number format required by the Gradio Audio player\n",
    "    waveform = audio_values[0].cpu().squeeze() * 2**15\n",
    "    return (sampling_rate, waveform.numpy().astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04ccc5-bede-4377-a47c-1eb9e2d0a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = utils.build_gr_blocks(generate)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af70fb-486c-4069-85f6-472e52278658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
