{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666a01df-d6f6-4909-8991-a9c231d38613",
   "metadata": {},
   "source": [
    "# Convert models to OpenVINO Intermediate representation (IR) format\n",
    "\n",
    "The OpenVINO model conversion API enables direct conversion of PyTorch models. We will utilize the `openvino.convert_model` method to acquire OpenVINO IR versions of the models. The method requires a model object and example input for model tracing. Under the hood, the converter will use the PyTorch JIT compiler, to build a frozen model graph.\n",
    "\n",
    "The pipeline consists of three important parts:\n",
    "\n",
    " - The [T5 text encoder](https://huggingface.co/google/flan-t5-base) that translates user prompts into vectors in the latent space that the next model - the MusicGen decoder can utilize.\n",
    " - The [MusicGen Language Model](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenForCausalLM) that auto-regressively generates audio tokens (codes).\n",
    " - The [EnCodec model](https://huggingface.co/facebook/encodec_24khz) (we will use only the decoder part of it) is used to decode the audio waveform from the audio tokens predicted by the MusicGen Language Model.\n",
    "\n",
    "### Dynamic shapes\n",
    "The text encoder can take in prompts of varying length. The MusicGen model is auto-regressive, meaning it takes in the encoded prompt along with the previously-generated token(s), which will grow with each successive pass. These are examples of dynamic shapes. Currently the NPU does not support dynamic shapes. And there may be other times you wish to convert dynamic shapes to static for performance reasons. So as we convert these models, we will also show strategies for converting dynamic shapes to static.\n",
    "\n",
    "Let us convert each model step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53843d6e-ade9-4efa-b5e1-6c3b1df368ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "import torch\n",
    "from torch.jit import TracerWarning\n",
    "from transformers import AutoProcessor, MusicgenProcessor, MusicgenForConditionalGeneration\n",
    "import gc\n",
    "import warnings\n",
    "# Ignore tracing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "original_model_dir = models_dir / \"musicgen-small\"\n",
    "processor_dir = models_dir / \"musicgen-small-processor\"\n",
    "t5_dynamic_ir_path = models_dir / \"t5.xml\"\n",
    "t5_static_ir_path = models_dir / \"t5_static.xml\"\n",
    "musicgen_0_ir_path = models_dir / \"mg_0.xml\"\n",
    "musicgen_ir_path = models_dir / \"mg.xml\"\n",
    "audio_decoder_dynamic_ir_path = models_dir / \"encodec.xml\"\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "loading_kwargs = {}\n",
    "# If transformers version is >= 4.40.0\n",
    "loading_kwargs[\"attn_implementation\"] = \"eager\" \n",
    "  \n",
    "# Load the pipeline\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(original_model_dir, torchscript=True, return_dict=False, **loading_kwargs)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # or use another specific tokenizer if necessary\n",
    "processor = MusicgenProcessor.from_pretrained(processor_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc835c-b13f-4e8c-9c87-3b67167319f7",
   "metadata": {},
   "source": [
    "Let's test the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa8613-8725-4b9a-9f96-6a62ce7bbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "sample_length = 8  # seconds\n",
    "\n",
    "frame_rate = model.config.audio_encoder.frame_rate\n",
    "n_tokens = sample_length * frame_rate + 3\n",
    "print(f\"Each second of output music requires generating {frame_rate} tokens\")\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "print(\"Sampling rate is\", sampling_rate, \"Hz\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval();\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "# Test the pipeline using the above prompt. Generate 8 seconds (the model generates 50 tokens for each second)\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=n_tokens)\n",
    "\n",
    "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de81f71-f8ae-4ca6-a91e-6c09e93341ad",
   "metadata": {},
   "source": [
    "### 1. Convert Text Encoder\n",
    "\n",
    "The text encoder is responsible for converting the input prompt, such as \"90s rock song with loud guitars and heavy drums\" into an embedding space that can be fed to the next model. Typically, it is a transformer-based encoder that maps a sequence of input tokens to a sequence of text embeddings.\n",
    "\n",
    "The input for the text encoder consists of a tensor `input_ids`, which contains token indices from the text processed by the tokenizer and `attention_mask` that we will ignore as we will process one prompt at a time and this vector will just consist of ones.\n",
    "\n",
    "We use OpenVINO Converter (OVC) below to convert the PyTorch model to the OpenVINO Intermediate Representation format (IR). First, convert with dynamically-shaped inputs, then read that model to learn which input dimensions are dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70534d14-9f85-420c-af41-ddf59b504e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_ov = ov.convert_model(model.text_encoder, example_input={\"input_ids\": inputs[\"input_ids\"]})\n",
    "ov.save_model(t5_ov, t5_dynamic_ir_path)\n",
    "\n",
    "t5_ov = core.read_model(t5_dynamic_ir_path)\n",
    "print(f\"Input shapes: {t5_ov.inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be948a62-cac2-4914-b00e-e850af17d21c",
   "metadata": {},
   "source": [
    "The `[?,?]` indicates that both dimensions are dynamic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bbcda-4e5e-49f2-ac04-ce7db1be46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input_ids shape = \", inputs.input_ids.shape)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60787f-7bc2-4597-a0e1-80280da63ff4",
   "metadata": {},
   "source": [
    "So the shapes of both inputs correspond to (batch size, sequence length), both of which are dynamic. A couple common approaches to converting models with dynamically-shaped inputs to statically-shaped are:\n",
    "1. Set a fixed max size and add padding. You can see this in the above example, where `input_ids[0]` is longer than `input_ids[1]` - it padded `input_ids[1]` with 0's to reach the same length as `input_ids[0]`. We can take this approach with our sequence length. Let's set it to be 100 tokens. Anything under 100 tokens will be padded with 0's. What about inputs longer than 100 tokens?\n",
    "2. Create multiple models with different statically-shaped inputs. This approach might be more practical for the batch size dimension, where we could create different models for different batch sizes. In this case, we will just create one model with batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d3c5e-aea5-4fcd-ab80-5cb9dd1e83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_input_layer = t5_ov.input(0)\n",
    "t5_output_layer = t5_ov.output(0)\n",
    "t5_ov.reshape({t5_input_layer.any_name: ov.PartialShape([1, 100])})\n",
    "t5_ov.validate_nodes_and_infer_types()\n",
    "ov.save_model(t5_ov, t5_static_ir_path)\n",
    "print(f\"Static input shapes: {t5_ov.inputs}\")\n",
    "del t5_ov\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9d03b-dd40-48fd-87b5-f640d8d10bdd",
   "metadata": {},
   "source": [
    "### 2. Convert MusicGen Language Model\n",
    "\n",
    "Skipping this step for this workshop. To learn more about converting this model for OpenVINO, see the OpenVINO notebook [Controllable Music Generation with MusicGen and OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/music-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755d9c0-8dcf-47be-a36b-8a327407a0c7",
   "metadata": {},
   "source": [
    "### 3. Convert Audio Decoder\n",
    "\n",
    "The audio decoder which is a part of the EnCodec model is used to recover the audio waveform from the audio tokens predicted by the MusicGen decoder. To learn more about the model please refer to the corresponding [OpenVINO example](../encodec-audio-compression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5620e-52fc-4868-b318-69a801092d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, output_ids):\n",
    "        return self.model.decode(output_ids, [None])\n",
    "\n",
    "audio_decoder_input = {\"output_ids\": torch.ones((1, 1, 4, n_tokens - 3), dtype=torch.int64)}\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_decoder_ov = ov.convert_model(AudioDecoder(model.audio_encoder), example_input=audio_decoder_input)\n",
    "ov.save_model(audio_decoder_ov, audio_decoder_dynamic_ir_path)\n",
    "\n",
    "print(f\"Audio Decoder Inputs:\\n{audio_decoder_ov.inputs}\")\n",
    "shapes = audio_decoder_input[\"output_ids\"].shape\n",
    "print(f\"output_ids shape = {shapes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd65e1-d215-4f5e-b5f5-27abda5a36cf",
   "metadata": {},
   "source": [
    "All the input dimensions are dynamic. Let's address each:\n",
    "* Batch size. We can set this to 1.\n",
    "* Number of channels. Our model is mono, so we can also set this to 1.\n",
    "* Number of codebooks. For our model, this is 4.\n",
    "* Sequence length (in tokens). This will vary. Remember, this model generates 50 tokens per second of audio. The two approaches we discussed when converting the t5 model were to set a max length and apply padding, or generate multiple versions of the model. Applying padding here is not practical, since we would have to generate extra silence then trim it. Instead we will generate multiple models, for outputs of 5s, 10s, and 20s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9898baf-6bdb-40b7-8908-816993f8c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_decoder_input_layer = audio_decoder_ov.input(0)\n",
    "\n",
    "for length in [5, 10, 20]:\n",
    "    n_tokens = length*frame_rate\n",
    "    audio_decoder_ov.reshape({audio_decoder_input_layer.any_name: ov.PartialShape([1, 1, 4, n_tokens])})\n",
    "    audio_decoder_ov.validate_nodes_and_infer_types()\n",
    "    ir_path = f\"{models_dir}/encodec_{length}s_ir.xml\"\n",
    "    print(f\"Saving to {ir_path}\")\n",
    "    ov.save_model(audio_decoder_ov, ir_path)\n",
    "\n",
    "del audio_decoder_ov\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e93a40-74f7-4cc9-a7e1-2cf9a7f10e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
