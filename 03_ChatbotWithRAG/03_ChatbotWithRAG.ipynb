{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02a561f4",
   "metadata": {},
   "source": [
    "# ChatBot without and with Retrieval, using OpenVINO and LangChain\n",
    "\n",
    "In this module, we will build up a ChatBot using LangChain. Then ask it questions about \"Lunar Lake\", which is the code name for Intel® Core™ Ultra Processors Series 2 laptop processors, which had not been released at the time the LLM was trained. Then we will build a vector database using a few documents that describe the \"Lunar Lake\" processors, and add retrieval-augmented generation (RAG) to the chatbot to ground its responses to this information.\n",
    "\n",
    "If you are running this on your own, not as part of a workshop, install packages using `requirements.txt` and run the `setup.py` script before using this notebook.\n",
    "\n",
    "This module's goal is to demonstrate the use case of building a chatbot using documents stored locally on an AI PC. It will be obvious that grounding the chatbot's responses to the documents is valuable, but the value of running locally is privacy and security - everything is local to the user's PC. The LLM is hard-coded to run on the GPU, the embeddings model on the NPU, and the reranker model uses the AUTO setting. Feel free to edit the code to try different settings. The LLM is Intel neural-chat-v3-3, but this has also been tested using TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
    "\n",
    "**Retrieval-augmented generation (RAG)** is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction) is a framework for developing applications powered by language models. It has a number of components specifically designed to help build RAG applications. In this tutorial, we’ll build a simple question-answering application over a text data source.\n",
    "\n",
    "In this example, the customized RAG pipeline consists of following components in order, where embedding, rerank and LLM will be deployed with OpenVINO to optimize their inference performance.\n",
    "\n",
    "![RAG](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/0076f6c7-75e4-4c2e-9015-87b355e5ca28)\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/llm-rag-langchain/llm-rag-langchain.ipynb\" />\n",
    "\n",
    "RAG description and image are courtesy of [Create a RAG system using OpenVINO and LangChain](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-rag-langchain).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c09cb8f",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import required dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b57cfb-e727-43a5-b2c9-8f1b1ba72061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    TextIteratorStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "\n",
    "import chatbot_utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79fe990a",
   "metadata": {},
   "source": [
    "### Load the LLM\n",
    "\n",
    "OpenVINO models can be run locally through the `HuggingFacePipeline` class in [LangChain](https://python.langchain.com/docs/integrations/llms/openvino/). To deploy a model with OpenVINO, you can specify the `backend=\"openvino\"` parameter to trigger OpenVINO as backend inference framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f708db-8de1-4efd-94b2-fcabc48d52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "model_dir = \"./models/neural-chat-7b-v3-3/INT4\"\n",
    "llm_device = \"GPU\"\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=str(model_dir),\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\n",
    "        \"device\": llm_device,\n",
    "        \"ov_config\": ov_config,\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    "    pipeline_kwargs={\"max_new_tokens\": 400},\n",
    ")\n",
    "# We'll define this later\n",
    "rag_chain = None\n",
    "\n",
    "if llm.pipeline.tokenizer.eos_token_id:\n",
    "    llm.pipeline.tokenizer.pad_token_id = llm.pipeline.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76450e3-84b4-45c1-967b-06ce8ee5960a",
   "metadata": {},
   "source": [
    "### Define Inference Function\n",
    "This shows how simple it is to build an inference pipeline using LangChain. Depending on whether we use RAG or not, we build different pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf3334-dea9-4e11-9400-e4efbf7751e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "def bot_inference(query, use_rag):\n",
    "    global db, llm, rag_chain\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        llm.pipeline.tokenizer,\n",
    "        timeout=60.0,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    pipeline_kwargs = dict(\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.6,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=10,\n",
    "        repetition_penalty=1.2,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    llm.pipeline_kwargs = pipeline_kwargs\n",
    "    if use_rag:\n",
    "        t1 = Thread(target=rag_chain.invoke, args=({\"input\": query},))\n",
    "    else:\n",
    "        input_text = utils.RAG_PROMPT_TEMPLATE.format(input=query, context=\"\")\n",
    "        t1 = Thread(target=llm.invoke, args=(input_text,))\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = utils.partial_text_processor(partial_text, new_text)\n",
    "        yield partial_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048837b-9403-4f26-86e4-4cb8377393ab",
   "metadata": {},
   "source": [
    "### Ask the Chatbot Questions\n",
    "\n",
    "Ask a question, choose a device to run the LLM on, then press Submit.\n",
    "\n",
    "Later on we will supply it with information on the recently-announced Intel Core Ultra 9 processor, code-named \"Lunar Lake\", which did not exist when the bot was trained. So ask it questions about Lunar Lake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c68e8-6126-4da5-9a85-3eb0955d6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_demo = utils.build_gr_blocks(bot_inference, use_rag=False)\n",
    "gr_demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fb8b0e4",
   "metadata": {},
   "source": [
    "## Run QA over Document\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "- **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happen offline.\n",
    "\n",
    "- **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "**Indexing**\n",
    "\n",
    "1. `Load`: First we need to load our data. We’ll use DocumentLoaders for this.\n",
    "2. `Split`: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t in a model’s finite context window.\n",
    "3. `Store`: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "![Indexing pipeline](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/dfed2ba3-0c3a-4e0e-a2a7-01638730486a)\n",
    "\n",
    "**Retrieval and generation**\n",
    "\n",
    "1. `Retrieve`: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "2. `Generate`: A LLM produces an answer using a prompt that includes the question and the retrieved data.\n",
    "\n",
    "![Retrieval and generation pipeline](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/f0545ddc-c0cd-4569-8c86-9879fdab105a)\n",
    "\n",
    "We can build a RAG pipeline of LangChain through [`create_retrieval_chain`](https://python.langchain.com/docs/modules/chains/), which will help to create a chain to connect RAG components including:\n",
    "\n",
    "- [`Vector stores`](https://python.langchain.com/docs/modules/data_connection/vectorstores/)，\n",
    "- [`Retrievers`](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "- [`LLM`](https://python.langchain.com/docs/integrations/llms/)\n",
    "- [`Embedding`](https://python.langchain.com/docs/integrations/text_embedding/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.embeddings import OpenVINOEmbeddings\n",
    "\n",
    "# Load and compile the embedding and rerank model\n",
    "embedding = utils.load_embeddings_model(\"NPU\")\n",
    "# Reranking improves the quality of retrieval by ranking the retrieved chunks according to a similarity score. \n",
    "# It adds some time to the inference step, because it is ranking the similarity to that specific query.\n",
    "reranker = utils.load_rerank_model(\"AUTO\")\n",
    "\n",
    "def build_vectordb(docs_path):\n",
    "    global llm, rag_chain\n",
    "    \n",
    "    print(f\"Building {docs_path}\")\n",
    "    # Create a loader to load all the .pdf files in a directory\n",
    "    loader = PyPDFDirectoryLoader(\"./local_docs\")\n",
    "    docs = loader.load()\n",
    "    # Split the docs into chunks, with some overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunked_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "    # Use the local copy of the model that's used if you're online and use embeddings_model = HuggingFaceEmbeddings()\n",
    "    embeddings_model = \"./models/bge-small-en-v1.5\"\n",
    "    db = FAISS.from_documents(chunked_docs, embedding)\n",
    "\n",
    "    # Number of results from the retrieval search to to use, and set the similarity score threshold\n",
    "    search_kwargs = {\"k\": 10, \"score_threshold\": 0.5}\n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=\"similarity_score_threshold\")\n",
    "    # Set up rerank, use the top 2 results\n",
    "    reranker.top_n = 2\n",
    "    # Define the retriever and prompt\n",
    "    retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=retriever)\n",
    "    prompt = PromptTemplate.from_template(utils.RAG_PROMPT_TEMPLATE)\n",
    "    # Add the retriever to the existing LLM chain\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "build_vectordb(\"./local_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908e5e9-4dcb-4fc8-8480-3cf70fd5e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_demo = utils.build_gr_blocks(bot_inference, use_rag=True)\n",
    "gr_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580a331-b97b-4673-9cd9-99aa3cf244ae",
   "metadata": {},
   "source": [
    "## Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like TinyLlama are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    " \n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/304aa048-f10c-41c6-bb31-6d2bfdf49cf5",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
