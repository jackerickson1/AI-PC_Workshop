{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37b660e4-3cba-412b-ab1a-b4acb88ff329",
   "metadata": {},
   "source": [
    "# Image generation with Latent Consistency Model and OpenVINO\n",
    "\n",
    "This module is based on the OpenVINO notebook [Image generation with Latent Consistency Model and OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/latent-consistency-models-image-generation)\n",
    "\n",
    "If you are running this on your own, not as part of a workshop, install packages using `requirements.txt` and run the `setup.py` script before using this notebook.\n",
    "\n",
    "This module enables experimentation of loading the Text Encoder, UNet, and VAE Decoder models on different devices to see the effects on the overall inference time for the pipeline. It loads the original LCM_Dreamshaper pipeline, saving certain components from it, then it builds an OpenVINO pipeline that combines those components with OpenVINO versions of the Text Encoder, UNet, and VAE Decoder. You can select the inference device for each of those models - if the selection changes, it rebuilds the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee66539d-99cc-45a3-80e4-4fbd6b520650",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6bc0c-fa5b-478b-ad44-b2f711497754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "import openvino.properties as properties\n",
    "from diffusers import DiffusionPipeline\n",
    "from pathlib import Path\n",
    "import os, gc, time\n",
    "import gradio as gr\n",
    "\n",
    "import OVLatentConsistencyModelPipeline as ov_lcm\n",
    "import LCM_utils as utils\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "model_path = \"models\"\n",
    "lcm_model_path = f\"{model_path}/LCM_Dreamshaper_v7\"\n",
    "TEXT_ENCODER_OV_PATH = Path(f\"{model_path}/text_encoder.xml\")\n",
    "UNET_OV_PATH = Path(f\"{model_path}/unet.xml\")\n",
    "VAE_DECODER_OV_PATH = Path(f\"{model_path}/vae_decoder.xml\")\n",
    "\n",
    "skip_safety_checker=False\n",
    "prev_text_enc_device = None\n",
    "prev_unet_device = None\n",
    "prev_vae_device = None\n",
    "ov_pipe = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d38976ba-29bf-49f2-8fc8-02d66f342d07",
   "metadata": {},
   "source": [
    "## Prepare inference pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "![lcm-pipeline](https://user-images.githubusercontent.com/29454499/277402235-079bacfb-3b6d-424b-8d47-5ddf601e1639.png)\n",
    "\n",
    "The pipeline takes a latent image representation and a text prompt is transformed to text embedding via CLIP's text encoder as an input. The initial latent image representation generated using random noise generator. In difference, with original Stable Diffusion pipeline, LCM also uses guidance scale for getting timestep conditional embeddings as input for diffusion process, while in Stable Diffusion, it used for scaling output latents.\n",
    "\n",
    "Next, the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. LCM introduces own scheduling algorithm that extends the denoising procedure introduced in denoising diffusion probabilistic models (DDPMs) with non-Markovian guidance.\n",
    "The *denoising* process is repeated given number of times (by default 50 in original SD pipeline, but for LCM small number of steps required ~2-8) to step-by-step retrieve better latent image representations.\n",
    "When complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c34ec-a373-41af-92e2-01fc101d675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    text_enc_device,\n",
    "    unet_device,\n",
    "    vae_device,\n",
    "    prompt: str,\n",
    "    num_inference_steps: int = 4,\n",
    "    progress=gr.Progress(track_tqdm=True),\n",
    "):\n",
    "    global ov_pipe\n",
    "    global text_enc_ov, unet_ov, vae_decoder_ov, prev_text_enc_device, prev_unet_device, prev_vae_device\n",
    "    \n",
    "    build_pipe = False\n",
    "    \n",
    "    seed = utils.randomize_seed_fn(seed=0, randomize_seed=True)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Compile each model to the specified device. If the model is already compiled to that device, then don't recompile\n",
    "    if text_enc_device != prev_text_enc_device:\n",
    "        text_enc_ov = core.compile_model(TEXT_ENCODER_OV_PATH, text_enc_device)\n",
    "        prev_text_enc_device = text_enc_device\n",
    "        build_pipe = True\n",
    "\n",
    "    if unet_device != prev_unet_device:\n",
    "        unet_ov = core.compile_model(UNET_OV_PATH, unet_device)         \n",
    "        prev_unet_device = unet_device\n",
    "        build_pipe = True\n",
    "\n",
    "    if vae_device != prev_vae_device:\n",
    "        ov_config = {\"INFERENCE_PRECISION_HINT\": \"f32\"} if vae_device != \"CPU\" else {}\n",
    "        vae_decoder_ov = core.compile_model(VAE_DECODER_OV_PATH, vae_device, ov_config)\n",
    "        prev_vae_device = vae_device\n",
    "        build_pipe = True\n",
    "\n",
    "    # Configure the pipeline, enabling the optional safety checker, which detects NSFW content\n",
    "    # This uses the above compiled models, and reuses the tokenizer, feature extractor, scheduler, \n",
    "    #   and safety checker from the original LCM pipeline\n",
    "    if build_pipe == True:\n",
    "        output_msg = \"(Re)building pipeline.\\n\"\n",
    "        pipe = DiffusionPipeline.from_pretrained(lcm_model_path)\n",
    "        scheduler = pipe.scheduler\n",
    "        tokenizer = pipe.tokenizer\n",
    "        feature_extractor = pipe.feature_extractor if not skip_safety_checker else None\n",
    "        safety_checker = pipe.safety_checker if not skip_safety_checker else None\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        ov_pipe = ov_lcm.OVLatentConsistencyModelPipeline(\n",
    "            tokenizer=tokenizer,\n",
    "            text_encoder=text_enc_ov,\n",
    "            unet=unet_ov,\n",
    "            vae_decoder=vae_decoder_ov,\n",
    "            scheduler=scheduler,\n",
    "            feature_extractor=feature_extractor,\n",
    "            safety_checker=safety_checker,\n",
    "        )\n",
    "    else:\n",
    "        output_msg = \"Running on existing pipeline.\\n\"\n",
    "        \n",
    "    output_msg = output_msg + f\"  Text Encoder running on {text_enc_ov.get_property(properties.execution_devices)}\\n\"\n",
    "    output_msg = output_msg + f\"  UNet running on {unet_ov.get_property(properties.execution_devices)}\\n\"\n",
    "    output_msg = output_msg + f\"  VAE Decoder running on {vae_decoder_ov.get_property(properties.execution_devices)}\\n\"    \n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = ov_pipe(\n",
    "        prompt=prompt,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        guidance_scale=8.0,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_images_per_prompt=1,\n",
    "        lcm_origin_steps=50,\n",
    "        output_type=\"pil\",\n",
    "    ).images\n",
    "    output_msg = output_msg + f\"\\nRan inference in {time.time() - start_time:.2f} seconds\"\n",
    "    for img in result:\n",
    "        time.sleep(1)\n",
    "        yield img[0], output_msg\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035dffa6-0ce9-4d22-8638-a2217d06e828",
   "metadata": {},
   "source": [
    "## Interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a4723-069c-439e-b093-dd1afa8512b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = utils.build_gr_blocks(generate)\n",
    "demo.queue().launch(share=False, inline=True, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586809a-0763-480f-b5d3-1bc033d88edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/latent-consistency-models-image-generation/latent-consistency-models-image-generation.png?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
